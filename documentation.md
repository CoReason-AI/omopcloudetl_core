### Documentation: omopcloudetl-core

This document provides detailed documentation for the `omopcloudetl-core` Python package. It outlines its architecture, core components, and its crucial role within the broader OmopCloudEtl ecosystem, based on the comprehensive design specification provided.

### 1\. Overview

The `omopcloudetl-core` package is the foundational library of the OmopCloudEtl ecosystem. This ecosystem is designed as a hyper-modular, cloud-native framework for executing petabyte-scale Extract, Transform, and Load (ETL) processes targeting the OHDSI OMOP Common Data Model (CDM).

The primary responsibility of `omopcloudetl-core` is to define the core abstractions, data models (Configuration, DML, Workflow), and the compilation logic for the ETL workflows.

The architecture employs a strict **Compiler/Submitter model** (also referred to as Compiler/Orchestrator Separation), which fundamentally decouples three key areas:

  * **Workflow Definition:** Defined in declarative YAML files.
  * **SQL Compilation:** Handled by `omopcloudetl-core`, translating definitions into dialect-specific SQL.
  * **Execution Orchestration:** Handled by pluggable orchestrator packages.

This separation addresses the scalability and maintainability bottlenecks often found in traditional, monolithic ETL tools. The package explicitly **does not handle** the actual execution of SQL or the movement of data.

-----

### 2\. Role in the OMOP ETL Ecosystem

`omopcloudetl-core` serves as the central definition and compilation engine that all other components interact with. It defines *what* to do, but not *how* to execute it. The ecosystem is composed of several distinct Python packages:

  * **Framework (Core and CLI)**
      * **`omopcloudetl-core`**: (This package) Defines abstractions, models, the `WorkflowCompiler`, and the `SpecificationManager`.
      * **`omopcloudetl`**: The Command Line Interface (CLI) that acts as the user-facing entry point and coordinator, used to initiate compilation and submit jobs.
  * **Assets**
      * **`omopcloudetl-omop-resources`**: Distributes standardized, dialect-agnostic YAML DML definitions and workflow structures.
  * **Orchestrators**
      * (e.g., `omopcloudetl-orchestrator-local`): Pluggable components responsible for executing the `CompiledWorkflowPlan` generated by the core. For example, the local orchestrator uses a simple thread pool for development and small-scale loads.
  * **Providers**
      * (e.g., `omopcloudetl-provider-databricks`, `-provider-postgres`): Pluggable components that provide connectivity (`BaseConnection`) and dialect-specific SQL generation logic (`BaseSQLGenerator`) for target database platforms.

**Interaction Flow:**

1.  A user defines a workflow using YAML assets.
2.  The CLI (`omopcloudetl`) invokes the `omopcloudetl-core` `WorkflowCompiler`.
3.  The `WorkflowCompiler` discovers the appropriate **Provider** package via the `DiscoveryManager`.
4.  The `Compiler` uses the Provider's `SQLGenerator` to compile the declarative DML into optimized, dialect-specific SQL.
5.  The `Compiler` produces a `CompiledWorkflowPlan`.
6.  The CLI submits this plan to the configured **Orchestrator** package for execution.

-----

### 3\. Key Architectural Principles (The Mandate)

The design of `omopcloudetl-core` mandates strict adherence to several principles crucial for scalability and cloud-native operation.

**3.1. Declarative DML over Imperative SQL**
Instead of writing complex, opaque, and often dialect-specific SQL scripts, users define transformation logic in structured, readable, and maintainable YAML DML files. `omopcloudetl-core` defines the Pydantic schema (`DMLDefinition`) for this structure, improving readability, validation, and portability.

**3.2. Compiler/Orchestrator Separation**
The core package acts strictly as a **compiler**. Its primary role is to take a user-defined workflow, validate its dependency graph (DAG), and compile it into an executable artifact called a `CompiledWorkflowPlan`. It contains **no execution logic**. The actual execution is delegated to a separate, pluggable Orchestrator.

**3.3. Decoupled Orchestration**
The core defines the `BaseOrchestrator` abstraction, allowing different execution environments to be used without modifying the core logic. This means the same workflow can be run on a local machine for testing and then scaled up on a cloud-native platform (e.g., Databricks Jobs, Argo Workflows) simply by changing the configuration.

**3.4. Zero Data Movement (Cloud-Native Ingress)**
A critical principle for cloud scalability. The framework is designed so that the client (the Python process) only moves instructions, never the actual data. The `BaseConnection.bulk_load` abstraction is strictly URI-based (e.g., `s3://`, `abfss://`), leveraging native database commands like `COPY INTO`. This avoids routing petabytes of data through a client machine, which is a common performance bottleneck.

**3.5. Automated Idempotency**
The `BaseSQLGenerator` abstraction requires that all implementations automatically produce idempotent SQL (e.g., `MERGE` statements or transactional `DELETE` + `INSERT`). This makes ETL pipelines resilient and safely re-runnable without causing data duplication or side effects.

**3.6. Dynamic CDM Specification Sourcing**
DDL definitions are not statically bundled. Instead, the `SpecificationManager` dynamically sources structural definitions from the official OHDSI CommonDataModel GitHub repository, ensuring alignment with the latest standards. It also supports caching to improve performance and can use local files for offline development.

**3.7. Commercial-Grade Robustness**
The framework includes mandatory connection retries (using `tenacity`) within the `BaseConnection` abstraction and structured observability through standardized `LoadMetrics` and `ExecutionMetrics` models, ensuring the framework is reliable and transparent.

-----

### 4\. Package Structure and Modules

The package is organized by functionality within the `src/omopcloudetl_core/` directory:

| Directory/Module | Description |
| :--- | :--- |
| **`abstractions/`** | Abstract Base Classes (ABCs) defining interfaces for the pluggable ecosystem (Connections, Generators, Orchestrators, Secrets). |
| **`compilation/`** | Contains the `WorkflowCompiler` (the core engine) and the `MetadataManager`. |
| **`config/`** | Manages project configuration loading, validation (`pydantic-settings`), and secrets resolution. |
| **`models/`** | Pydantic models defining the structure of DML (`dml.py`), Workflows (`workflow.py`), and Metrics (`metrics.py`). |
| **`specifications/`** | The `SpecificationManager` for dynamic sourcing of OMOP CDM DDL from GitHub. |
| **`discovery.py`** | The `DiscoveryManager` for locating and loading Providers and Orchestrators via entry points. |
| **`exceptions.py`** | Custom exception classes (e.g., `CompilationError`, `ConfigurationError`). |
| **`sql_tools.py`** | Utilities for Jinja rendering, SQL parsing (`sqlparse`), and query tagging. |

-----

### 5\. Core Components and Functionality

**5.1. Abstractions**
These interfaces define the contracts between the Core and the pluggable components.

  * **`BaseConnection` (abstractions/connections.py):** Defines connectivity. Key features include mandatory `tenacity` retries in `connect()`, and the URI-based `bulk_load()` method (enforcing Zero Data Movement). It also exposes a `ScalabilityTier` enum to advise users on platform suitability.
  * **`BaseSQLGenerator` (abstractions/generators.py):** Compiles `DMLDefinition` into dialect-specific SQL. The `generate_transform_sql` method must produce idempotent SQL (enforcing Automated Idempotency).
  * **`BaseDDLGenerator` (abstractions/generators.py):** Generates DDL statements based on the `CDMSpecification`.
  * **`BaseOrchestrator` (abstractions/orchestrators.py):** Defines the contract for executing a `CompiledWorkflowPlan`.

**5.2. Declarative DML Schema (models/dml.py)**
The core defines the Pydantic schema for transformations. The `DMLDefinition` model includes:

  * Target and Source definitions.
  * `idempotency_keys`: Critical fields used by the `BaseSQLGenerator` for `MERGE` operations.
  * `mappings`: A discriminated union defining data mapping logic:
      * `DirectMapping`: Column-to-column.
      * `ExpressionMapping`: A SQL expression.
      * `ConstantMapping`: A static value.

**5.3. Workflow Models and Compilation**

  * **`WorkflowConfig` (models/workflow.py):** The input definition (DAG), containing steps (DML, BulkLoad, DDL) and their dependencies.
  * **`CompiledWorkflowPlan` (models/workflow.py):** The output artifact of the compiler. This executable plan contains fully rendered SQL (`CompiledSQLStep`) or resolved bulk load instructions (`CompiledBulkLoadStep`).
  * **`WorkflowCompiler` (compilation/compiler.py):** The central engine. It validates the DAG (using `networkx`), initializes the appropriate Generators, iterates through the steps, compiles DML/DDL into SQL, resolves URIs, applies query tags, and outputs the `CompiledWorkflowPlan`.

**5.4. Specification Manager (specifications/)**
Implements Dynamic CDM Sourcing. The `SpecificationManager` fetches OMOP CDM structural definitions from the OHDSI GitHub repository. It uses `requests` for fetching, `tenacity` for resilience, and `diskcache` for caching the results locally. It parses the raw data into a standardized `CDMSpecification` model.

**5.5. Discovery Manager (discovery.py)**
Manages the discovery and instantiation of pluggable components using Python's `importlib.metadata.entry_points` mechanism. It discovers components via specific entry point groups:

  * `omopcloudetl.providers`
  * `omopcloudetl.orchestrators`
  * `omopcloudetl.secrets`

-----

### 6\. Installation and Dependencies

`omopcloudetl-core` requires Python 3.10 or later.

```bash
pip install omopcloudetl-core
```

Key dependencies managed via Poetry include:

  * **`pydantic` (\>=2.0)** and **`pydantic-settings`**: Configuration and data modeling.
  * **`PyYAML`**: Parsing workflow and DML definitions.
  * **`Jinja2`**: Template rendering.
  * **`networkx`**: DAG validation.
  * **`tenacity`**: Resilience framework (retries).
  * **`requests`** and **`diskcache`**: Fetching and caching CDM specifications.
  * **`importlib-metadata`**: Plugin discovery.

-----

### 7\. Codebase Comparison to Specification

The `omopcloudetl-core` codebase is a complete and faithful implementation of the 8-phase build specification. It successfully embodies all the architectural principles and component responsibilities outlined.

| Specification Phase | Component | Implemented In | Status & Notes |
| :--- | :--- | :--- | :--- |
| **Phase 0: Setup** | Project Structure & Dependencies | `pyproject.toml`, `src/omopcloudetl_core/` | **Complete**. All specified dependencies and the full directory structure are in place. |
| **Phase 1: Core Infra** | Custom Exceptions & Logging | `exceptions.py`, `logging.py` | **Complete**. A full hierarchy of custom exceptions and a centralized logger are implemented. |
| **Phase 2: Config** | Secrets Abstraction & Config Models | `abstractions/secrets.py`, `config/models.py`, `config/manager.py` | **Complete**. `ConfigManager` properly loads YAML, resolves secrets via the `DiscoveryManager`, and populates Pydantic models. |
| **Phase 3: Core Models** | Metrics & DML Schema | `models/metrics.py`, `models/dml.py` | **Complete**. The Pydantic models for observability (`LoadMetrics`) and the declarative DML schema (using discriminated unions) are fully defined. |
| **Phase 4: Abstractions**| `BaseConnection`, `BaseSQLGenerator`, `BaseOrchestrator` | `abstractions/` | **Complete**. All core abstract base classes are defined as per the specification, enforcing the key architectural patterns. |
| **Phase 5: CDM Sourcing**| `SpecificationManager` & Models | `specifications/` | **Complete**. The manager correctly fetches, parses, and caches OMOP CDM specifications from the OHDSI repository. |
| **Phase 6: Discovery** | `DiscoveryManager` | `discovery.py` | **Complete**. The manager uses `importlib.metadata` to dynamically discover and instantiate pluggable providers and orchestrators from other packages. |
| **Phase 7: Workflow** | Workflow Models & Compiled Plan | `models/workflow.py` | **Complete**. The models for both the user-facing `WorkflowConfig` and the compiler's output, `CompiledWorkflowPlan`, are fully implemented. |
| **Phase 8: Compilation**| `WorkflowCompiler` & `sql_tools` | `compilation/compiler.py`, `sql_tools.py` | **Complete**. The `WorkflowCompiler` successfully orchestrates all other components to translate a high-level workflow into a detailed, executable plan without containing any execution logic itself. |

In summary, the `omopcloudetl-core` package serves as the robust, well-architected foundation of the entire ecosystem. It successfully translates the high-level design principles into a concrete, testable, and extensible software artifact, setting the stage for the broader ecosystem of providers and orchestrators to build upon.